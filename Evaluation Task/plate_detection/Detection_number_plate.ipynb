{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import resnet18\nfrom PIL import Image\nimport pandas as pd\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:54:04.236779Z","iopub.execute_input":"2023-07-30T11:54:04.237352Z","iopub.status.idle":"2023-07-30T11:54:04.255300Z","shell.execute_reply.started":"2023-07-30T11:54:04.237321Z","shell.execute_reply":"2023-07-30T11:54:04.253806Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef load_annotations(yolo_annotations_folder):\n    annotations = []\n    for filename in os.listdir(yolo_annotations_folder):\n        if filename.endswith(\".txt\"):\n            image_filename = os.path.splitext(filename)[0]  # Remove the extension if it exists\n            image_filename += \".JPG\"  # Add the extension back\n\n            with open(os.path.join(yolo_annotations_folder, filename), 'r') as file:\n                for line in file:\n                    class_label, x, y, width, height = line.strip().split()\n                    x, y, width, height = float(x), float(y), float(width), float(height)\n                    class_label = int(class_label)  # Convert class label to integer\n\n                    annotations.append((image_filename, x, y, width, height, class_label))\n    return annotations\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:54:10.688571Z","iopub.execute_input":"2023-07-30T11:54:10.688952Z","iopub.status.idle":"2023-07-30T11:54:10.696749Z","shell.execute_reply.started":"2023-07-30T11:54:10.688923Z","shell.execute_reply":"2023-07-30T11:54:10.695720Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Implement data augmentation and preprocessing (resize and normalization)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:54:43.810794Z","iopub.execute_input":"2023-07-30T11:54:43.811227Z","iopub.status.idle":"2023-07-30T11:54:43.818246Z","shell.execute_reply.started":"2023-07-30T11:54:43.811173Z","shell.execute_reply":"2023-07-30T11:54:43.816833Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Create a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, annotations, image_folder, transform=None):\n        self.annotations = annotations\n        self.image_folder = image_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        image_filename, x, y, width, height, class_label = self.annotations[index]\n        image_path = os.path.join(self.image_folder, image_filename)\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, torch.tensor([x, y, width, height]), torch.tensor(class_label)\n\n# Load your dataset\nannotations = load_annotations('/kaggle/input/annotation-plates/obj_train_data')\nimage_folder = '/kaggle/input/pakistani-number-plates1/Cars'\ndataset = CustomDataset(annotations, image_folder, transform=transform)\n\n# Split dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:54:48.818674Z","iopub.execute_input":"2023-07-30T11:54:48.819908Z","iopub.status.idle":"2023-07-30T11:54:48.887886Z","shell.execute_reply.started":"2023-07-30T11:54:48.819860Z","shell.execute_reply":"2023-07-30T11:54:48.886941Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"annotations","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:54:53.940097Z","iopub.execute_input":"2023-07-30T11:54:53.940520Z","iopub.status.idle":"2023-07-30T11:54:53.957333Z","shell.execute_reply.started":"2023-07-30T11:54:53.940482Z","shell.execute_reply":"2023-07-30T11:54:53.956334Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[('DSC_1047.JPG', 0.585955, 0.675205, 0.139755, 0.11728, 0),\n ('DSC_1027.JPG', 0.662659, 0.688763, 0.117313, 0.108255, 0),\n ('DSC_0995.JPG', 0.611427, 0.642829, 0.157466, 0.088885, 0),\n ('DSC_1002.JPG', 0.447084, 0.620645, 0.158837, 0.095267, 0),\n ('DSC_1037.JPG', 0.612611, 0.707584, 0.150373, 0.12178, 0),\n ('DSC_1066.JPG', 0.651747, 0.671544, 0.131674, 0.118782, 0),\n ('DSC_1089.JPG', 0.590535, 0.641472, 0.147953, 0.124786, 0),\n ('DSC_1089.JPG', 0.93685, 0.416691, 0.04493, 0.045105, 0),\n ('DSC_1089.JPG', 0.065581, 0.389631, 0.03398, 0.036089, 0),\n ('DSC_0973.JPG', 0.452083, 0.695282, 0.1743, 0.104757, 0),\n ('DSC_1041.JPG', 0.533733, 0.673084, 0.162803, 0.109763, 0),\n ('DSC_1052.JPG', 0.584708, 0.634732, 0.137894, 0.084197, 0),\n ('DSC_1033.JPG', 0.653379, 0.65053, 0.128697, 0.109755, 0),\n ('DSC_0988.JPG', 0.636738, 0.706389, 0.121065, 0.101587, 0),\n ('DSC_1106.JPG', 0.591448, 0.656197, 0.133508, 0.085924, 0),\n ('DSC_1096.JPG', 0.60783, 0.697769, 0.150895, 0.100744, 0),\n ('DSC_1054.JPG', 0.568307, 0.740748, 0.162588, 0.115777, 0),\n ('DSC_0998.JPG', 0.420297, 0.653131, 0.155379, 0.093645, 0),\n ('DSC_1007.JPG', 0.638464, 0.705589, 0.12961, 0.106356, 0),\n ('DSC_1032.JPG', 0.511361, 0.717361, 0.156295, 0.108258, 0),\n ('DSC_1022.JPG', 0.521046, 0.70614, 0.165382, 0.10676, 0),\n ('DSC_1014.JPG', 0.477535, 0.66354, 0.143127, 0.107929, 0),\n ('DSC_0996.JPG', 0.510439, 0.704033, 0.180697, 0.109525, 0),\n ('DSC_1020.JPG', 0.613036, 0.674537, 0.12574, 0.100741, 0),\n ('DSC_0992.JPG', 0.610354, 0.650775, 0.157887, 0.098413, 0),\n ('DSC_1018.JPG', 0.609333, 0.606059, 0.128843, 0.078187, 0),\n ('DSC_1059.JPG', 0.485511, 0.767789, 0.168165, 0.100737, 0),\n ('DSC_1003.JPG', 0.546857, 0.718324, 0.166907, 0.077779, 0),\n ('DSC_0983.JPG', 0.551336, 0.63814, 0.117916, 0.085718, 0),\n ('DSC_0989.JPG', 0.655429, 0.680896, 0.145918, 0.120611, 0),\n ('DSC_1060.JPG', 0.621685, 0.653423, 0.141641, 0.109756, 0),\n ('DSC_1026.JPG', 0.581073, 0.704523, 0.159781, 0.112765, 0),\n ('DSC_1067.JPG', 0.643906, 0.65874, 0.127782, 0.10224, 0),\n ('DSC_0977.JPG', 0.617999, 0.66038, 0.16258, 0.114289, 0),\n ('DSC_0982.JPG', 0.59, 0.732525, 0.121054, 0.111114, 0),\n ('DSC_1087.JPG', 0.565628, 0.733864, 0.160169, 0.11427, 0),\n ('DSC_1039.JPG', 0.60817, 0.65724, 0.146297, 0.10825, 0),\n ('DSC_1091.JPG', 0.513702, 0.718125, 0.129477, 0.058641, 0),\n ('DSC_1045.JPG', 0.477065, 0.685021, 0.158588, 0.091715, 0),\n ('DSC_0994.JPG', 0.66404, 0.604716, 0.148687, 0.107977, 0),\n ('DSC_1030.JPG', 0.573963, 0.645131, 0.141088, 0.066154, 0),\n ('DSC_1015.JPG', 0.679827, 0.656536, 0.124794, 0.097727, 0),\n ('DSC_1081.JPG', 0.563926, 0.715863, 0.092347, 0.054129, 0),\n ('DSC_1001.JPG', 0.826861, 0.619824, 0.141648, 0.07459, 0),\n ('DSC_1034.JPG', 0.542588, 0.70315, 0.148392, 0.115771, 0),\n ('DSC_1079.JPG', 0.577196, 0.711407, 0.163932, 0.075182, 0),\n ('DSC_1099.JPG', 0.56463, 0.712149, 0.142055, 0.105246, 0),\n ('DSC_1023.JPG', 0.574094, 0.664068, 0.147968, 0.109755, 0),\n ('DSC_1074.JPG', 0.554295, 0.740704, 0.165427, 0.109762, 0),\n ('DSC_1082.JPG', 0.59424, 0.678263, 0.155415, 0.111259, 0),\n ('DSC_1035.JPG', 0.573107, 0.640764, 0.170659, 0.105241, 0),\n ('DSC_0997.JPG', 0.55126, 0.642911, 0.113193, 0.101588, 0),\n ('DSC_1103.JPG', 0.487068, 0.687777, 0.156114, 0.103462, 0),\n ('DSC_1072.JPG', 0.621654, 0.607688, 0.152621, 0.102241, 0),\n ('DSC_0985.JPG', 0.561163, 0.642083, 0.155462, 0.096823, 0),\n ('DSC_1009.JPG', 0.578544, 0.692141, 0.137059, 0.10159, 0),\n ('DSC_0969.JPG', 0.541162, 0.673857, 0.154393, 0.103114, 0),\n ('DSC_1029.JPG', 0.63975, 0.694011, 0.126808, 0.109762, 0),\n ('DSC_1038.JPG', 0.52436, 0.744415, 0.178319, 0.126295, 0),\n ('DSC_1040.JPG', 0.566306, 0.700768, 0.15307, 0.099238, 0),\n ('DSC_1084.JPG', 0.630727, 0.718818, 0.154637, 0.120283, 0),\n ('DSC_1094.JPG', 0.568515, 0.724943, 0.171538, 0.121783, 0),\n ('DSC_1098.JPG', 0.645499, 0.702321, 0.138073, 0.09773, 0),\n ('DSC_1092.JPG', 0.538524, 0.739137, 0.163079, 0.11728, 0),\n ('DSC_1092.JPG', 0.08771, 0.490301, 0.076883, 0.070668, 0),\n ('DSC_1008.JPG', 0.74797, 0.467442, 0.108376, 0.103172, 0),\n ('DSC_1107.JPG', 0.526775, 0.71671, 0.173097, 0.105212, 0),\n ('DSC_0984.JPG', 0.629067, 0.696088, 0.131593, 0.122222, 0),\n ('DSC_1010.JPG', 0.585488, 0.64279, 0.142568, 0.092066, 0),\n ('DSC_1055.JPG', 0.585912, 0.733154, 0.165167, 0.133819, 0),\n ('DSC_1105.JPG', 0.590533, 0.692077, 0.158595, 0.105217, 0),\n ('DSC_1028.JPG', 0.54782, 0.757194, 0.155315, 0.127799, 0),\n ('DSC_1053.JPG', 0.599365, 0.673817, 0.134623, 0.072176, 0),\n ('DSC_0975.JPG', 0.597249, 0.763563, 0.170857, 0.060321, 0),\n ('DSC_1070.JPG', 0.570015, 0.685861, 0.14196, 0.126299, 0),\n ('DSC_1080.JPG', 0.445646, 0.719627, 0.161069, 0.085698, 0),\n ('DSC_1062.JPG', 0.561569, 0.736987, 0.163198, 0.102246, 0),\n ('DSC_1076.JPG', 0.676198, 0.770748, 0.168414, 0.130808, 0),\n ('DSC_0968.JPG', 0.504814, 0.649135, 0.153791, 0.104772, 0),\n ('DSC_1064.JPG', 0.594824, 0.679088, 0.151324, 0.121792, 0),\n ('DSC_1051.JPG', 0.560645, 0.648979, 0.135808, 0.112765, 0),\n ('DSC_1048.JPG', 0.460927, 0.70907, 0.176465, 0.109759, 0),\n ('DSC_1006.JPG', 0.596005, 0.698342, 0.133153, 0.101593, 0),\n ('DSC_1049.JPG', 0.514296, 0.727912, 0.18569, 0.111259, 0),\n ('DSC_0990.JPG', 0.491089, 0.741299, 0.170197, 0.098409, 0),\n ('DSC_1093.JPG', 0.647965, 0.640763, 0.144298, 0.118777, 0),\n ('DSC_1090.JPG', 0.636933, 0.657988, 0.123396, 0.112776, 0),\n ('DSC_1085.JPG', 0.695752, 0.679103, 0.130441, 0.112767, 0),\n ('DSC_1056.JPG', 0.458159, 0.753388, 0.181475, 0.123291, 0),\n ('DSC_0971.JPG', 0.487785, 0.680965, 0.180752, 0.082509, 0),\n ('DSC_1019.JPG', 0.52745, 0.679774, 0.148509, 0.105251, 0),\n ('DSC_1104.JPG', 0.522267, 0.719376, 0.180789, 0.07892, 0),\n ('DSC_0999.JPG', 0.567475, 0.614358, 0.116503, 0.088892, 0),\n ('DSC_1069.JPG', 0.724777, 0.678324, 0.114269, 0.102237, 0),\n ('DSC_1075.JPG', 0.488451, 0.696375, 0.175526, 0.108259, 0),\n ('DSC_1097.JPG', 0.610733, 0.661767, 0.132376, 0.100742, 0),\n ('DSC_1057.JPG', 0.656173, 0.721937, 0.148693, 0.123293, 0),\n ('DSC_0979.JPG', 0.693474, 0.669864, 0.120498, 0.095233, 0),\n ('DSC_0970.JPG', 0.577651, 0.66595, 0.133835, 0.077757, 0),\n ('DSC_1068.JPG', 0.601232, 0.721126, 0.174401, 0.112767, 0),\n ('DSC_1058.JPG', 0.596129, 0.662448, 0.138456, 0.100736, 0),\n ('DSC_1071.JPG', 0.484434, 0.747389, 0.18232, 0.12329, 0),\n ('DSC_1011.JPG', 0.553441, 0.614313, 0.148513, 0.104767, 0),\n ('DSC_1100.JPG', 0.613814, 0.649704, 0.144542, 0.099237, 0)]"},"metadata":{}}]},{"cell_type":"code","source":"image_folder","metadata":{"execution":{"iopub.status.busy":"2023-07-30T07:19:43.984579Z","iopub.execute_input":"2023-07-30T07:19:43.984937Z","iopub.status.idle":"2023-07-30T07:19:43.991395Z","shell.execute_reply.started":"2023-07-30T07:19:43.984900Z","shell.execute_reply":"2023-07-30T07:19:43.990017Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/pakistani-number-plates/Cars'"},"metadata":{}}]},{"cell_type":"code","source":"# Modify ResNet18 for object detection by adding custom regression and classification heads\nclass ResNet18ObjectDetection(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18ObjectDetection, self).__init__()\n        self.resnet18 = resnet18(pretrained=True)\n        in_features = self.resnet18.fc.in_features\n        self.resnet18.fc = nn.Linear(in_features, num_classes)\n\n        # Add additional layers for bounding box regression\n        self.bbox_regression = nn.Linear(in_features, 4)  # 4 for (x, y, width, height)\n\n    def forward(self, x):\n        x = self.resnet18(x)\n        class_scores = x\n        bbox_regression = self.bbox_regression(x)\n        return class_scores, bbox_regression\n\n# Initialize the model\nnum_classes = 1  # Assuming only one class (number plate)\nmodel = ResNet18ObjectDetection(num_classes)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T09:15:02.561381Z","iopub.execute_input":"2023-07-30T09:15:02.561738Z","iopub.status.idle":"2023-07-30T09:15:02.777404Z","shell.execute_reply.started":"2023-07-30T09:15:02.561686Z","shell.execute_reply":"2023-07-30T09:15:02.775889Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\n# Modify ResNet18 for object detection by adding custom regression and classification heads\nclass ResNet18ObjectDetection(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18ObjectDetection, self).__init__()\n        self.resnet18 = models.resnet18(pretrained=True)\n        in_features = self.resnet18.fc.in_features\n\n        # Classification head\n        self.classification_head = nn.Linear(in_features, num_classes)\n\n        # Bounding box regression head\n        self.bbox_regression_head = nn.Linear(in_features, 4)  # 4 for (x, y, width, height)\n\n    def forward(self, x):\n        x = self.resnet18.conv1(x)\n        x = self.resnet18.bn1(x)\n        x = self.resnet18.relu(x)\n        x = self.resnet18.maxpool(x)\n\n        x = self.resnet18.layer1(x)\n        x = self.resnet18.layer2(x)\n        x = self.resnet18.layer3(x)\n        x = self.resnet18.layer4(x)\n\n        # Global average pooling\n        x = self.resnet18.avgpool(x)\n        x = x.view(x.size(0), -1)\n\n        class_scores = self.classification_head(x)\n        bbox_regression = self.bbox_regression_head(x)\n\n        return class_scores, bbox_regression\n\n# Initialize the model\nnum_classes = 1  # Assuming only one class (number plate)\nmodel = ResNet18ObjectDetection(num_classes)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T11:55:04.931721Z","iopub.execute_input":"2023-07-30T11:55:04.932127Z","iopub.status.idle":"2023-07-30T11:55:05.168221Z","shell.execute_reply.started":"2023-07-30T11:55:04.932093Z","shell.execute_reply":"2023-07-30T11:55:05.167272Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 2\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for images, targets, labels in train_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        class_scores, bbox_regression = model(images)\n\n        # Calculate losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        # Backpropagation and optimization\n        total_loss.backward()\n        optimizer.step()\n\n    scheduler.step()\n    \n    \n\n    # Validation after each epoch (optional)\n    model.eval()\n    val_class_loss = 0.0\n    val_bbox_loss = 0.0\n    total_val_samples = 0\n\n    with torch.no_grad():\n        for val_images, val_targets, val_labels in val_loader:\n            val_images, val_targets, val_labels = val_images.to(device), val_targets.to(device), val_labels.to(device)\n            val_class_scores, val_bbox_regression = model(val_images)\n\n            # Calculate validation losses\n            val_class_loss += criterion(val_class_scores, val_labels).item()\n            val_bbox_loss += bbox_criterion(val_bbox_regression, val_targets).item()\n\n            total_val_samples += val_labels.size(0)\n\n    # Average validation losses\n    avg_val_class_loss = val_class_loss / total_val_samples\n    avg_val_bbox_loss = val_bbox_loss / total_val_samples\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss.item():.4f}, \"\n          f\"Val Class Loss: {avg_val_class_loss:.4f}, Val BBox Loss: {avg_val_bbox_loss:.4f}\")\n\n# Training is complete\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T10:07:28.685403Z","iopub.execute_input":"2023-07-30T10:07:28.685772Z","iopub.status.idle":"2023-07-30T10:07:56.706239Z","shell.execute_reply.started":"2023-07-30T10:07:28.685744Z","shell.execute_reply":"2023-07-30T10:07:56.705270Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 3/3, Train Loss: 0.0006, Val Accuracy: 1.0000, Val BBox Loss: 0.0001\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize early stopping variables\nbest_val_loss = float('inf')\npatience = 3  # Number of epochs to wait for improvement\nearly_stopping_counter = 0\n\n\n# Set up loss function, optimizer, and learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\nbbox_criterion = nn.SmoothL1Loss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_class_loss = 0.0\n    train_bbox_loss = 0.0\n\n    for images, targets, labels in train_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        class_scores, bbox_regression = model(images)\n\n        # Calculate losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        # Backpropagation and optimization\n        total_loss.backward()\n        optimizer.step()\n\n        train_class_loss += class_loss.item()\n        train_bbox_loss += bbox_loss.item()\n\n    avg_train_class_loss = train_class_loss / len(train_loader)\n    avg_train_bbox_loss = train_bbox_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss.item():.4f}, \"\n          f\"Train Class Loss: {avg_train_class_loss:.4f}, Train BBox Loss: {avg_train_bbox_loss:.4f}\")\n\n    scheduler.step()\n\n    # Validation after each epoch\n    model.eval()\n    val_class_loss = 0.0\n    val_bbox_loss = 0.0\n    total_val_samples = 0\n\n    with torch.no_grad():\n        for val_images, val_targets, val_labels in val_loader:\n            val_images, val_targets, val_labels = val_images.to(device), val_targets.to(device), val_labels.to(device)\n            val_class_scores, val_bbox_regression = model(val_images)\n\n            # Calculate validation losses\n            val_class_loss += criterion(val_class_scores, val_labels).item()\n            val_bbox_loss += bbox_criterion(val_bbox_regression, val_targets).item()\n\n            total_val_samples += val_labels.size(0)\n\n    # Average validation losses\n    avg_val_class_loss = val_class_loss / total_val_samples\n    avg_val_bbox_loss = val_bbox_loss / total_val_samples\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Val Class Loss: {avg_val_class_loss:.4f}, Val BBox Loss: {avg_val_bbox_loss:.4f}\")\n\n    # Check for early stopping\n    val_loss = avg_val_class_loss + avg_val_bbox_loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter >= patience:\n        print(\"Early stopping triggered. Training stopped.\")\n        break\n\n# Training is complete\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T10:16:08.444085Z","iopub.execute_input":"2023-07-30T10:16:08.444471Z","iopub.status.idle":"2023-07-30T10:17:50.779602Z","shell.execute_reply.started":"2023-07-30T10:16:08.444441Z","shell.execute_reply":"2023-07-30T10:17:50.778677Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.0170, Train Class Loss: 0.0000, Train BBox Loss: 0.0783\nEpoch 1/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0373\nEpoch 2/10, Train Loss: 0.0206, Train Class Loss: 0.0000, Train BBox Loss: 0.0362\nEpoch 2/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0439\nEpoch 3/10, Train Loss: 0.0033, Train Class Loss: 0.0000, Train BBox Loss: 0.0054\nEpoch 3/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0177\nEpoch 4/10, Train Loss: 0.0035, Train Class Loss: 0.0000, Train BBox Loss: 0.0042\nEpoch 4/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0034\nEpoch 5/10, Train Loss: 0.0019, Train Class Loss: 0.0000, Train BBox Loss: 0.0031\nEpoch 5/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0036\nEpoch 6/10, Train Loss: 0.0025, Train Class Loss: 0.0000, Train BBox Loss: 0.0054\nEpoch 6/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0009\nEpoch 7/10, Train Loss: 0.0073, Train Class Loss: 0.0000, Train BBox Loss: 0.0049\nEpoch 7/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0070\nEpoch 8/10, Train Loss: 0.0016, Train Class Loss: 0.0000, Train BBox Loss: 0.0027\nEpoch 8/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0014\nEpoch 9/10, Train Loss: 0.0015, Train Class Loss: 0.0000, Train BBox Loss: 0.0024\nEpoch 9/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0001\nEpoch 10/10, Train Loss: 0.0023, Train Class Loss: 0.0000, Train BBox Loss: 0.0026\nEpoch 10/10, Val Class Loss: 0.0000, Val BBox Loss: 0.0001\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_class_loss = 0.0\n    train_bbox_loss = 0.0\n    detected_regions = []  # List to store the detected regions for each image\n\n    for images, targets, labels in train_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        class_scores, bbox_regression = model(images)\n\n        # Calculate losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        # Backpropagation and optimization\n        total_loss.backward()\n        optimizer.step()\n\n        train_class_loss += class_loss.item()\n        train_bbox_loss += bbox_loss.item()\n\n        # Post-processing to get detected regions (assuming class_scores contains confidence scores)\n        conf_scores, pred_boxes = torch.max(class_scores, dim=1)\n        detected_images_regions = []\n        for i in range(images.size(0)):\n            if conf_scores[i] > confidence_threshold:  # Set a threshold for confidence score\n                x, y, width, height = pred_boxes[i]  # Assuming pred_boxes contains predicted bounding boxes\n                detected_images_regions.append((x.item(), y.item(), width.item(), height.item()))\n        detected_regions.extend(detected_images_regions)\n\n    avg_train_class_loss = train_class_loss / len(train_loader)\n    avg_train_bbox_loss = train_bbox_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss.item():.4f}, \"\n          f\"Train Class Loss: {avg_train_class_loss:.4f}, Train BBox Loss: {avg_train_bbox_loss:.4f}\")\n\n    scheduler.step()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize early stopping variables\n# best_val_loss = float('inf')\n# patience = 3  # Number of epochs to wait for improvement\n# early_stopping_counter = 0\n\n\n# Set up loss function, optimizer, and learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\nbbox_criterion = nn.SmoothL1Loss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nconfidence_threshold = 0.5\n# Rest of your code...\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_class_loss = 0.0\n    train_bbox_loss = 0.0\n    detected_regions = []  # List to store the detected regions for each image\n\n    for images, targets, labels in train_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        class_scores, bbox_regression = model(images)\n\n        # Calculate losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        # Backpropagation and optimization\n        total_loss.backward()\n        optimizer.step()\n\n        train_class_loss += class_loss.item()\n        train_bbox_loss += bbox_loss.item()\n\n        # Post-processing to get detected regions (assuming class_scores contains confidence scores)\n        conf_scores, pred_boxes = torch.max(class_scores, dim=1)\n        detected_images_regions = []\n        for i in range(images.size(0)):\n            if conf_scores[i] > confidence_threshold:  # Set a threshold for confidence score\n                x, y, width, height = pred_boxes[i]  # Assuming pred_boxes contains predicted bounding boxes\n                detected_images_regions.append((x.item(), y.item(), width.item(), height.item()))\n        detected_regions.extend(detected_images_regions)\n\n    avg_train_class_loss = train_class_loss / len(train_loader)\n    avg_train_bbox_loss = train_bbox_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss.item():.4f}, \"\n          f\"Train Class Loss: {avg_train_class_loss:.4f}, Train BBox Loss: {avg_train_bbox_loss:.4f}\")\n\n    scheduler.step()\n\n    # Validation after each epoch\n    model.eval()\n    val_class_loss = 0.0\n    val_bbox_loss = 0.0\n    total_val_samples = 0\n    detected_regions_val = []  # List to store the detected regions for each validation image\n\n    with torch.no_grad():\n        for val_images, val_targets, val_labels in val_loader:\n            val_images, val_targets, val_labels = val_images.to(device), val_targets.to(device), val_labels.to(device)\n            val_class_scores, val_bbox_regression = model(val_images)\n\n            # Calculate validation losses\n            val_class_loss += criterion(val_class_scores, val_labels).item()\n            val_bbox_loss += bbox_criterion(val_bbox_regression, val_targets).item()\n\n            total_val_samples += val_labels.size(0)\n\n            # Post-processing to get detected regions in validation (assuming class_scores contains confidence scores)\n            conf_scores_val, pred_boxes_val = torch.max(val_class_scores, dim=1)\n            detected_images_regions_val = []\n            for i in range(val_images.size(0)):\n                if conf_scores_val[i] > confidence_threshold:  # Set a threshold for confidence score\n                    x_val, y_val, width_val, height_val = pred_boxes_val[i]  # Assuming pred_boxes contains predicted bounding boxes\n                    detected_images_regions_val.append((x_val.item(), y_val.item(), width_val.item(), height_val.item()))\n            detected_regions_val.extend(detected_images_regions_val)\n\n    # Average validation losses\n    avg_val_class_loss = val_class_loss / total_val_samples\n    avg_val_bbox_loss = val_bbox_loss / total_val_samples\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Val Class Loss: {avg_val_class_loss:.4f}, Val BBox Loss: {avg_val_bbox_loss:.4f}\")\n\n    # Check for early stopping\n    val_loss = avg_val_class_loss + avg_val_bbox_loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n        patience =3\n\n        # Save the best model\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter >= patience:\n        print(\"Early stopping triggered. Training stopped.\")\n        break\n\n# Training is complete\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-30T12:01:33.900756Z","iopub.execute_input":"2023-07-30T12:01:33.901166Z","iopub.status.idle":"2023-07-30T12:01:39.084493Z","shell.execute_reply.started":"2023-07-30T12:01:33.901133Z","shell.execute_reply":"2023-07-30T12:01:39.083284Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf_scores[i] \u001b[38;5;241m>\u001b[39m confidence_threshold:  \u001b[38;5;66;03m# Set a threshold for confidence score\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m         x, y, width, height \u001b[38;5;241m=\u001b[39m pred_boxes[i]  \u001b[38;5;66;03m# Assuming pred_boxes contains predicted bounding boxes\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         detected_images_regions\u001b[38;5;241m.\u001b[39mappend((x\u001b[38;5;241m.\u001b[39mitem(), y\u001b[38;5;241m.\u001b[39mitem(), width\u001b[38;5;241m.\u001b[39mitem(), height\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     50\u001b[0m detected_regions\u001b[38;5;241m.\u001b[39mextend(detected_images_regions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n","\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"],"ename":"TypeError","evalue":"iteration over a 0-d tensor","output_type":"error"}]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_class_loss = 0.0\n    train_bbox_loss = 0.0\n    detected_regions = []  # List to store the detected regions for each image\n\n    for images, targets, labels in train_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        class_scores, bbox_regression = model(images)\n\n        # Calculate losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        # Backpropagation and optimization\n        total_loss.backward()\n        optimizer.step()\n\n        train_class_loss += class_loss.item()\n        train_bbox_loss += bbox_loss.item()\n\n        # Post-processing to get detected regions (assuming class_scores contains confidence scores)\n        conf_scores, pred_boxes = torch.max(class_scores, dim=1)\n        detected_images_regions = []\n        for i in range(images.size(0)):\n            # Extract the indices of elements greater than the confidence threshold\n            detected_indices = torch.nonzero(conf_scores[i] > confidence_threshold).squeeze(1)\n            for idx in detected_indices:\n                x, y, width, height = pred_boxes[i][idx]\n                detected_images_regions.append((x.item(), y.item(), width.item(), height.item()))\n        detected_regions.extend(detected_images_regions)\n\n    avg_train_class_loss = train_class_loss / len(train_loader)\n    avg_train_bbox_loss = train_bbox_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss.item():.4f}, \"\n          f\"Train Class Loss: {avg_train_class_loss:.4f}, Train BBox Loss: {avg_train_bbox_loss:.4f}\")\n\n    scheduler.step()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# After the training loop, you can use the 'detected_regions' list for OCR on the training data\n\n# Perform OCR on the detected number plate regions in the training data\ntrain_ocr_results = ocr_on_number_plates(detected_regions, train_images)\n\n# Display OCR results for the training data\nfor i, results in enumerate(train_ocr_results):\n    print(f\"OCR Results for Training Image {i+1}:\")\n    for (x, y, width, height, plate_text) in results:\n        print(f\"Plate Coordinates: (x={x}, y={y}), Width: {width}, Height: {height}\")\n        print(\"Plate Text:\", plate_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_class_loss = 0.0\ntest_bbox_loss = 0.0\ntotal_test_samples = 0\n\nwith torch.no_grad():\n    for images, targets, labels in test_loader:\n        images, targets, labels = images.to(device), targets.to(device), labels.to(device)\n        class_scores, bbox_regression = model(images)\n\n        # Calculate test losses\n        class_loss = criterion(class_scores, labels)\n        bbox_loss = bbox_criterion(bbox_regression, targets)\n        total_loss = class_loss + bbox_loss\n\n        test_class_loss += class_loss.item()\n        test_bbox_loss += bbox_loss.item()\n\n        total_test_samples += labels.size(0)\n\n# Average test losses\navg_test_class_loss = test_class_loss / total_test_samples\navg_test_bbox_loss = test_bbox_loss / total_test_samples\n\nprint(f\"Test Class Loss: {avg_test_class_loss:.4f}, Test BBox Loss: {avg_test_bbox_loss:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}